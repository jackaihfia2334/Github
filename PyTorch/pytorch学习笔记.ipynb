{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31699c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "a = t.arange(12).view(3,4)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a0164a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 10])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[t.tensor([1, 2]), t.tensor([0, 2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0a85633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  8],\n",
       "        [ 6, 10]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[t.tensor([1,2])[None,:], t.tensor([0, 2])[:,None]] # 不相同形状的index索引，满足广播法则\n",
    "                                                     # 获取索引为[1,0]、[2,0]、[1,2]、[2,2]的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67ef25ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2]]),\n",
       " tensor([[0],\n",
       "         [2]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.tensor([1,2])[None,:], t.tensor([0, 2])[:,None]   #先广播法则扩展，再索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a2f0ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8,  9, 10, 11],\n",
      "        [ 0,  1,  2,  3]])\n"
     ]
    }
   ],
   "source": [
    "print(a[[2,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a431b261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7, 10,  1])\n"
     ]
    }
   ],
   "source": [
    "print(a[[1, 2, 0], [3, 2, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbc3535d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, -3,  2],\n",
      "        [ 2,  9, -1],\n",
      "        [-8,  4,  1]])\n",
      "tensor([[ 2, -3,  4],\n",
      "        [ 4, 18, -1],\n",
      "        [-8,  8,  2]])\n"
     ]
    }
   ],
   "source": [
    "b = t.tensor([[1, -3, 2], [2, 9, -1], [-8, 4, 1]])\n",
    "print(b)\n",
    "b[b > 0] *= 2\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aebac351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([[[1, 3, 3, 5],\n",
      "         [5, 4, 5, 1],\n",
      "         [4, 5, 2, 5]],\n",
      "\n",
      "        [[6, 8, 0, 1],\n",
      "         [3, 8, 1, 8],\n",
      "         [1, 4, 6, 5]],\n",
      "\n",
      "        [[3, 1, 4, 3],\n",
      "         [5, 7, 5, 6],\n",
      "         [7, 6, 1, 4]],\n",
      "\n",
      "        [[3, 2, 5, 4],\n",
      "         [6, 7, 8, 2],\n",
      "         [4, 0, 8, 3]],\n",
      "\n",
      "        [[2, 5, 4, 5],\n",
      "         [4, 6, 3, 6],\n",
      "         [4, 4, 8, 1]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 随机生成一组形状为(5,3,4)、0~9数字组成的张量\n",
    "a = t.randint(1, 10, (5,3,4))\n",
    "# 获取输出的unique list和索引\n",
    "output, inverse_indices = t.unique(a, return_inverse=True)\n",
    "print(output)\n",
    "print(inverse_indices)\n",
    "# 通过整数数组索引 还原原始tensor\n",
    "a_generate = output[inverse_indices]\n",
    "a_generate.equal(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6c8192c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 2, 1, 9, 3, 4, 7]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[9, 3, 2, 1]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.randint(1, 10, (1,7))\n",
    "print(x)\n",
    "x[:,t.tensor([3,4,1,2])]  #  整数数组索引，输出的size等于索引的size，可以大于x本身的size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9412061c",
   "metadata": {},
   "source": [
    "# nn.embedding学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e4e6eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n",
      "tensor([[1, 2],\n",
      "        [4, 2],\n",
      "        [4, 1],\n",
      "        [3, 2]])\n",
      "----------------------------------------------------------------------\n",
      "torch.Size([4, 2, 6])\n",
      "tensor([[[-0.4663,  0.5425,  0.0533, -1.5261,  0.3110,  0.1871],\n",
      "         [-1.0019, -1.4731,  0.5315, -0.0386,  2.1083,  0.2508]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0019, -1.4731,  0.5315, -0.0386,  2.1083,  0.2508]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.4663,  0.5425,  0.0533, -1.5261,  0.3110,  0.1871]],\n",
      "\n",
      "        [[ 2.3033, -0.0814, -0.1821, -1.6813, -1.2886, -0.6068],\n",
      "         [-1.0019, -1.4731,  0.5315, -0.0386,  2.1083,  0.2508]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# 输入\n",
    "x = torch.LongTensor([[1, 2], [4, 2],[4, 1],[3, 2]])\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "# 调用nn.Embedding函数\n",
    "# 一共5个词，每个词的词向量维度设置为6维\n",
    "embeddings = nn.Embedding(5, 6, padding_idx=4)\n",
    "print(embeddings(x).shape)\n",
    "print(embeddings(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc301f1",
   "metadata": {},
   "source": [
    "# BCELoss()学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e3508aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6702, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "0.6702415347099304\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "m = nn.Sigmoid()\n",
    "loss = nn.BCELoss()\n",
    "input = torch.randn(8, requires_grad=True)\n",
    "target = torch.empty(8).random_(2)\n",
    "output = loss(m(input), target)\n",
    "print(output)\n",
    "print(output.item())    #item()取出对应位置元素的值\n",
    "\n",
    "#only one element tensors can be converted to Python scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15dfac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
