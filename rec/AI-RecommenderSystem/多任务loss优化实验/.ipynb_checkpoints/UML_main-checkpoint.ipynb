{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4b72fd7",
   "metadata": {},
   "source": [
    "## Description:\n",
    "这个是sharedBottom模型的demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "324f6ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:\n",
      "DeepCTR version 0.9.0 detected. Your version is 0.8.2.\n",
      "Use `pip install -U deepctr` to upgrade.Changelog: https://github.com/shenweichen/DeepCTR/releases/tag/v0.9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ZHONGQ~1\\AppData\\Local\\Temp/ipykernel_62380/3089501287.py:23: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ZHONGQ~1\\AppData\\Local\\Temp/ipykernel_62380/3089501287.py:23: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, mean_absolute_error\n",
    "from deepctr.feature_column import SparseFeat, VarLenSparseFeat, DenseFeat\n",
    "from deepctr.feature_column import get_feature_names\n",
    "from SharedBottom import SharedBottom\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True      # TensorFlow按需分配显存\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5  # 指定显存分配比例\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "577efc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data_process'\n",
    "data = pd.read_csv(os.path.join(data_path, 'train_data.csv'), index_col=0, parse_dates=['expo_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88a84ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择出需要用到的列\n",
    "use_cols = ['user_id', 'article_id', 'expo_time', 'net_status', 'exop_position', 'duration', 'device', 'city', 'age', 'gender', 'img_num', 'cat_1', 'click']\n",
    "data_new = data[use_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce2ea472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于这个data_new的数据量还是太大， 我电脑训练不动， 所以这里再进行一波抽样\n",
    "users = set(data_new['user_id'])\n",
    "sampled_users = random.sample(users, 1000)\n",
    "data_new = data_new[data_new['user_id'].isin(sampled_users)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0b628c",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e380f114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理img_num\n",
    "def transform(x):\n",
    "    if x == '上海':\n",
    "        return 0\n",
    "    elif isinstance(x, float):\n",
    "        return float(x)\n",
    "    else:\n",
    "        return float(eval(x))\n",
    "data_new['img_num'] = data_new['img_num'].apply(lambda x: transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46f77eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_raw = data_new[['user_id']].drop_duplicates('user_id')\n",
    "doc_id_raw = data_new[['article_id']].drop_duplicates('article_id')\n",
    "\n",
    "# 简单数据预处理\n",
    "sparse_features = [\n",
    "    'user_id', 'article_id', 'net_status', 'exop_position', 'device', 'city', 'age', 'gender', 'cat_1'\n",
    "]\n",
    "dense_features = [\n",
    "    'img_num'\n",
    "]\n",
    "\n",
    "# 填充缺失值\n",
    "data_new[sparse_features] = data_new[sparse_features].fillna('-1')\n",
    "data_new[dense_features] = data_new[dense_features].fillna(0)\n",
    "\n",
    "# 归一化\n",
    "mms = MinMaxScaler(feature_range=(0, 1))\n",
    "data_new[dense_features] = mms.fit_transform(data_new[dense_features])\n",
    "\n",
    "feature_max_idx = {}\n",
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data_new[feat] = lbe.fit_transform(data_new[feat])\n",
    "    feature_max_idx[feat] = data_new[feat].max() + 1000\n",
    "\n",
    "# 构建用户id词典和doc的id词典，方便从用户idx找到原始的id\n",
    "# user_id_enc = data[['user_id']].drop_duplicates('user_id')\n",
    "# doc_id_enc = data[['article_id']].drop_duplicates('article_id')\n",
    "# user_idx_2_rawid = dict(zip(user_id_enc['user_id'], user_id_raw['user_id']))\n",
    "# doc_idx_2_rawid = dict(zip(doc_id_enc['article_id'], doc_id_raw['article_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad2691ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集  这里按照曝光时间划分\n",
    "train_data = data_new[data_new['expo_time'] < '2021-07-06']\n",
    "test_data = data_new[data_new['expo_time'] >= '2021-07-06']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad199918",
   "metadata": {},
   "source": [
    "## 特征封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0e5ea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_feature_columns = [SparseFeat(feat, feature_max_idx[feat], embedding_dim=4) for feat in sparse_features]\n",
    "Dense_feature_columns = [DenseFeat(feat, 1) for feat in dense_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9f538ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分dnn和linear特征\n",
    "dnn_features_columns = sparse_feature_columns + Dense_feature_columns\n",
    "lhuc_feature_columns = sparse_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5355501",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = get_feature_names(dnn_features_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6a5f286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AttributeError: 'numpy.dtype[int64]' object has no attribute 'base_dtype' \n",
    "# Keras需要把输入声明为Keras张量，其他的比如numpy张量作为输入不好使\n",
    "train_model_input = {name: tf.keras.backend.constant(train_data[name]) for name in feature_names}\n",
    "test_model_input = {name: tf.keras.backend.constant(test_data[name]) for name in feature_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4803b7",
   "metadata": {},
   "source": [
    "## 模型搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b0270fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SharedBottom(dnn_features_columns, lhuc_feature_columns, tower_dnn_hidden_units=[], task_types=['regression', 'binary'], \n",
    "             task_names=['duration', 'click'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7b39f2",
   "metadata": {},
   "source": [
    "## 模型的训练和预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f373ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_duration = tf.keras.backend.constant(train_data['duration'].values)\n",
    "label_click = tf.keras.backend.constant(train_data['click'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01764354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建数据管道\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_model_input, (label_duration, label_click))).shuffle(buffer_size=100).batch(128).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9d77f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练这里，需要用到底层的训练脚本，这里不能用高层keras的API\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_reg_loss = tf.keras.metrics.Mean(name='train_reg_loss')\n",
    "train_bin_loss = tf.keras.metrics.Mean(name='train_bin_loss')\n",
    "loss_func = {\"binary\": tf.keras.losses.binary_crossentropy, \"regression\": tf.keras.losses.mean_squared_error}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae2f0ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(features, labels, task_types, weight=[0.02, 0.98]):\n",
    "    losses = []\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # 遍历每个任务\n",
    "        for i, task_type in enumerate(task_types):\n",
    "            out = model(features, training=True)\n",
    "            task_loss = loss_func[task_types[i]](out[i], labels[i])\n",
    "\n",
    "            losses.append(weight[i] * task_loss)\n",
    "            \n",
    "        # 直接求和 这里可以手动指定权重\n",
    "        loss = tf.add_n(losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_reg_loss(losses[0])\n",
    "    train_bin_loss(losses[1])\n",
    "    return loss, losses[0], losses[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2130117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 140.83474731445312 - regression_loss: 161.18621826171875 - binary_loss: 5.314276218414307\n",
      "Epoch 2, Loss: 138.92530822753906 - regression_loss: 161.39010620117188 - binary_loss: 5.497527599334717\n",
      "Epoch 3, Loss: 138.42620849609375 - regression_loss: 126.88057708740234 - binary_loss: 5.864028453826904\n",
      "Epoch 4, Loss: 138.05685424804688 - regression_loss: 111.17665100097656 - binary_loss: 4.764523029327393\n",
      "Epoch 5, Loss: 137.8128204345703 - regression_loss: 127.75333404541016 - binary_loss: 4.947774410247803\n",
      "Epoch 6, Loss: 137.6511993408203 - regression_loss: 109.87477111816406 - binary_loss: 5.314276218414307\n",
      "Epoch 7, Loss: 137.55368041992188 - regression_loss: 237.552490234375 - binary_loss: 5.864028453826904\n",
      "Epoch 8, Loss: 137.40977478027344 - regression_loss: 117.3536605834961 - binary_loss: 6.047279357910156\n",
      "Epoch 9, Loss: 137.3338623046875 - regression_loss: 121.86355590820312 - binary_loss: 5.314275741577148\n",
      "Epoch 10, Loss: 137.24070739746094 - regression_loss: 92.81501770019531 - binary_loss: 4.214771270751953\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "best_test_loss = float('inf')\n",
    "task_types = [\"regression\", \"binary\"]\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss.reset_states()\n",
    "    train_reg_loss.reset_states()\n",
    "    train_bin_loss.reset_states()\n",
    "\n",
    "    for feature, labels in train_ds:\n",
    "        loss, loss_reg, loss_bin = train_step(feature, labels, task_types)\n",
    "\n",
    "    template = 'Epoch {}, Loss: {} - regression_loss: {} - binary_loss: {}'\n",
    "    print(template.format(epoch, train_loss.result(), \n",
    "                          np.mean(loss_reg), \n",
    "                         np.mean(loss_bin)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d671e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ans = model.predict(test_model_input, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32dc4aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test click AUC 0.47\n"
     ]
    }
   ],
   "source": [
    "print(\"test click AUC\", round(roc_auc_score(test_data['click'], pred_ans[1]), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c377e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test duration 31.7404\n"
     ]
    }
   ],
   "source": [
    "print(\"test duration\", round(mean_absolute_error(test_data['duration'], pred_ans[0]), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adb3df7",
   "metadata": {},
   "source": [
    "如果不指定权重， 就会发现， 回归任务训练的较好， 而分类任务有些差。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efe45df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test click AUC 0.4914\n",
      "test duration 30.4913\n"
     ]
    }
   ],
   "source": [
    "pred_ans = model.predict(test_model_input, batch_size=256)\n",
    "print(\"test click AUC\", round(roc_auc_score(test_data['click'], pred_ans[1]), 4))\n",
    "print(\"test duration\", round(mean_absolute_error(test_data['duration'], pred_ans[0]), 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
