# 记录学习rec中阅读过的好文章



### 推荐算法中的五环之歌https://zhuanlan.zhihu.com/p/336643635

**记忆与扩展**：推荐系统的永恒主题

**Embedding**：无中生有，将推荐算法从“精确匹配”转化为“模糊查找，提升扩展能力

**高维、稀疏的类别特征**：是推荐算法的“一等公民”，即便是实数特征，我们也经常将其分桶，离散化成类别特征

- 推荐算法的基础是画像
- 现实场景中，“目标~特征”之间鲜有线性关系
- 线上工程实现，更偏爱高维、稀疏、离散的类别特征

**特征交叉**：单个特征的表达能力太弱，所以需要交叉多个特征来增强模型的表达能力。

**Field & Pooling**：Field——举用户历史的例子，“用户观看历史”是Field，看过的每个视频的DocId是Feature

属于一个Field的各Feature Embedding通过Pooling压缩成一个向量，以减少DNN的规模，多个Field Embedding拼接在一起，喂入DNN完成特征之间的高阶交叉

- 为了增强推荐算法的扩展性，我们需要将类别特征先进行Embedding，再接入DNN，进行高阶特征交叉。但是怎么接入DNN，变成了一个问题。
- 推荐算法的特征空间有上亿级别，每维特征再embedding成一个向量。如果将这些向量拼接起来接入DNN，DNN的输入层恐怕就要上十亿、百亿的规模，对于存储、计算都会造成不可想像的压力。

**总结推荐算法中的经典套路**：

- 排序模型一般都衍生自Google的Wide & Deep模型，有一个浅层模型（LR或FM）负责记忆，DNN负责扩展
- 特征一般都采用类别特征。画像、用户历史天然就是高维、稀疏的类别特征。对于实数型特征，比如用户、物料的一些统计指标，在我的实践中，也通过分桶，先离散化成类别特征，再接入模型
- 每个类别特征经过Embedding变成一个向量，以扩展其内涵。
- 属于一个Field的各Feature Embedding通过Pooling压缩成一个向量，以减少DNN的规模
- 多个Field Embedding拼接在一起，喂入DNN
- DNN通过多层Fully Connection Layer (FC)完成特征之间的高阶交叉，增强模型的扩展能力。
