# 推荐系统入门

## 一、概述

### 推荐系统意义

个性化推荐系统通过分析用户的行为日志，得到用户当前的甚至未来可能的兴趣，给不同的用户展示不同的(个性化)的页面，来提高网站或者app的点击率、转化率、留存率等指标。

搜索和推荐都是解决互联网大数据时代信息过载的手段，但是它们也存在着许多的不同：

用户意图；个性化程度；优化目标；马太效应和长尾理论；

### 推荐系统架构

<u>架构设计的核心在于平衡和妥协</u>

思考推荐系统架构考虑的第一个问题是**确定边界**：知道推荐系统要负责哪部分问题，这就是边界内的部分。在这个基础上，架构要分为哪几个部分，每一部分需要完成的子功能是什么，每一部分依赖外界的什么。

**系统架构：**

设计思想是大数据背景下如何有效利用海量和实时数据，将推荐系统按照对数据利用情况和系统响应要求出发，将整个架构分为**离线层、近线层、在线层**三个模块。系统架构是如何权衡利弊，如何利用各种技术工具帮助我们达到想要的目的的，方便我们理解为什么推荐系统要这样设计。

离线层对于数据数量和算法复杂度限制更少，没有很强的时间要求。由于没有及时加入最新的数据，所以很容易过时。在线层能更快地响应最近的事件和用户交互，但必须实时完成，这会限制使用算法的复杂性和处理的数据量。近线层介于两种方法之间。

1. 离线层：不用实时数据，不提供实时响应；
2. 近线层：使用实时数据，不保证实时响应；
3. 在线层：使用实时数据，保证实时在线服务；

![在这里插入图片描述](http://ryluo.oss-cn-chengdu.aliyuncs.com/%E5%9B%BE%E7%89%87image-20220409204658032.png)

整个数据部分其实是一整个链路，主要是三块：

客户端及服务器实时数据处理：记录实时数据；流处理平台准实时数据处理：记录准实时数据，在推荐领域基本上只有一个类别，就是用户行为数据；大数据平台离线数据处理：所有“脏活累活”复杂的操作都是在离线完成的。

**算法架构：**

**召回、粗排、排序、重排**等算法环节角度出发的，重要的是要去理解每个环节需要完成的任务，每个环节的评价体系，以及为什么要那么设计。还有一个重要问题是每个环节涉及到的技术栈和主流算法。这种角度来看是把推荐系统从前往后串起来，**其中每一个模块既有在离线层工作的，也有在在线层工作的**。

![在这里插入图片描述](http://ryluo.oss-cn-chengdu.aliyuncs.com/%E5%9B%BE%E7%89%87image-20220409211354342.png)

召回阶段我们现在主要是在保证Item质量的基础上注重覆盖率多样性，粗排阶段主要用简单的模型来解决不同路的召回和当前用户的相关性问题，最后截断到1k个以内的候选集。

粗排的原因是有时候召回的结果还是太多，精排层速度还是跟不上，所以加入粗排。粗排可以理解为精排前的一轮过滤机制，减轻精排模块的压力。粗排介于召回和精排之间，要同时兼顾精准性和低延迟。

精排是推荐系统各层级中最纯粹的一层，他的目标比较单一且集中，一门心思的实现目标的调优即可。解决样本规模和时效性问题。

重排序阶段对精排生成的Top-N个物品的序列进行重新排序，生成一个Top-K个物品的序列，作为排序系统最后的结果，直接展现给用户。重排序的原因是因为多个物品之间往往是相互影响的，而精排序是根据PointWise得分，容易造成推荐结果同质化严重，有很多冗余信息。

**常见的有三种优化目标：Point Wise、Pair Wise 和 List Wise。**

### 推荐系统技术栈

完整的一套推荐系统体系里，不仅会涉及到推荐算法工程师、后台开发工程师、数据挖掘/分析工程师、NLP/CV工程师还有前端、客户端甚至产品、运营等支持。作为算法工程师，需要掌握的技术栈主要就是**算法和工程**。

#### 算法：

**召回**：轻量快速低延迟，不需要十分准确，但不可遗漏。**粗排**：介于召回和精排之间，要同时兼顾精准性和低延迟。一般模型也不能过于复杂。**精排**：在最大时延允许的情况下，保证精确性。精排系统构建一般需要涉及样本、特征、模型三部分。**重排**：获取精排的排序结果，基于运营策略、多样性、context上下文等，进行一个微调。重排中规则比较多，但目前也有不少基于模型来提升重排效果的方案。**混排**：多个业务线都想在Feeds流中获取曝光，则需要对它们的结果进行混排。

**画像层**：算法主要体现在如何绘制一个用户画像和商品画像。用户画像是大家比较容易理解的，比如用户年龄、爱好通常APP会通过注册界面收集这些信息内容画像各家的做法也不同，当前比较主流的都会涉及到一个**多模态信息内容理解**。一般推荐系统会加入多模态的一个内容理解。

。。。。。省略

#### 工程：

- **编程语言**：Python、Java（scala）、C++、sql、shell；
- **机器学习**：Tensorflow/Pytorch、GraphLab/GraphCHI、LGB/Xgboost、SKLearn；
- **数据分析**：Pandas、Numpy、Seaborn、Spark；
- 数据存储：mysql、redis、mangodb、hive、kafka、es、hbase；
- 相似计算：annoy、faiss、kgraph
- 流计算：Spark Streaming、Flink
- 分布式：Hadoop、Spark

## 二、推荐系统算法基础

### 经典召回模型

#### 算法评估

**召回率**：在模型召回预测的物品中，预测准确的物品占用户实际喜欢的物品的比例。

**精确率**：推荐的物品中，对用户准确推荐的物品占总物品的比例。

- 如要确保召回率高，一般是推荐更多的物品，期望推荐的物品中会涵盖用户喜爱的物品。而实际中，推荐的物品中用户实际喜爱的物品占少数，推荐的精确率就会很低。故同时要确保高召回率和精确率往往是矛盾的，所以实际中需要在二者之间进行权衡。

**覆盖率**：推荐系统能够推荐出来的物品占总物品集合的比例。

- 覆盖率表示最终的推荐列表中包含多大比例的物品。如果所有物品都被给推荐给至少一个用户， 那么覆盖率是100%。

**新颖度**：用推荐列表中物品的平均流行度度量推荐结果的新颖度。 如果推荐出的物品都很热门， 说明推荐的新颖度较低。

#### 协同过滤算法

##### 基本思想

根据用户之前的喜好以及其他兴趣相近的用户的选择来给用户推荐物品。

基于对用户历史行为数据的挖掘发现用户的喜好偏向， 并预测用户可能喜好的产品进行推荐。

一般是**仅仅基于用户的行为数据**（评价、购买、下载等）, 而不依赖于项的任何附加信息（物品自身特征）或者用户的任何附加信息（年龄， 性别等）

- 基于用户的协同过滤算法（UserCF）
- 基于物品的协同过滤算法（ItemCF）

重点是**计算相似度**     实现代码见jupyter notebook   **rec_test**

##### 相似性度量方法

1. **杰卡德（Jaccard）相似系数**     适用于隐式反馈数据（0-1）

2. **余弦相似度**     衡量了两个向量的夹角，夹角越小越相似   在度量文本相似度、用户相似度、物品相似度的时候都较为常用。            from sklearn.metrics.pairwise importcosine_similarity

3. **皮尔逊相关系数**  就是概率论中的相关系数，对协方差归一化得到，范围在 −1 到 1
   
   - 相关度量的是两个变量的变化趋势是否一致，两个随机变量是不是同增同减。
   - 不适合用作计算布尔值向量（0-1）之间相关度。
   
   from  scipy.stats  import  pearsonr

### UserCF算法（基于用户的协同过滤算法）

计算过程：

1.计算用户之间的相似度 2.计算用户对新物品的评分预测 3.对用户进行物品推荐

具体过程和实现代码可见  jupyter notebook   **rec_test**

存在的问题：

1.数据稀疏性

- 一个大型的电子商务推荐系统一般有非常多的物品，用户可能买的其中不到1%的物品，不同用户之间买的物品重叠性较低，导致算法无法找到一个用户的邻居，即偏好相似的用户。
- 这导致UserCF不适用于那些正反馈获取较困难的应用场景(如酒店预订， 大件物品购买等低频应用)。

2.算法扩展性

- 基于用户的协同过滤需要维护用户相似度矩阵以便快速的找出 TopN*T**o**pN* 相似用户， 该矩阵的存储开销非常大，存储空间随着用户数量的增加而增加。
- 故不适合用户数据量大的情况使用

### UserCF算法（基于用户的协同过滤算法）

ItemCF算法并不利用物品的内容属性计算物品之间的相似度， 主要通过分析用户的行为记录计算物品之间的相似度， 该算法认为， 物品 A 和物品 C 具有很大的相似度是因为喜欢物品 A 的用户极可能喜欢物品 C。

和基于内容的推荐算法(Content-Based Recommendation)进行区分！

### 协同过滤算法的问题分析

泛化能力弱：

- 即协同过滤无法将两个物品相似的信息推广到其他物品的相似性上。
- 导致的问题是**热门物品具有很强的头部效应， 容易跟大量物品产生相似， 而尾部物品由于特征向量稀疏， 导致很少被推荐**。

![图片](http://ryluo.oss-cn-chengdu.aliyuncs.com/JavaxxhHm3BAtMfsy2AV.png!thumbnail)

- 可以看出，D 是一件热门物品，其与 A、B、C 的相似度比较大。因此，推荐系统更可能将 D推荐给用过 A、B、C的用户。
- 但是，推荐系统无法找出 A,B,C之间相似性的原因是交互数据太稀疏， 缺乏相似性计算的直接数据。

所以这就是协同过滤的天然缺陷：**推荐系统头部效应明显， 处理稀疏向量的能力弱**。

为了解决这个问题， 同时增加模型的泛化能力。2006年，**矩阵分解技术(Matrix Factorization, MF**)被提出：

- 该方法在协同过滤共现矩阵的基础上， 使用**更稠密的隐向量**表示用户和物品， 挖掘用户和物品的隐含兴趣和隐含特征。
- 在一定程度上弥补协同过滤模型处理稀疏矩阵能力不足的问题。

**1.什么时候使用UserCF，什么时候使用ItemCF？为什么？**

> （1）UserCF
> 
> - 由于是基于用户相似度进行推荐， 所以具备更强的社交特性， 这样的特点非常适于**用户少， 物品多， 时效性较强的场合**。
>   - 比如新闻推荐场景， 因为新闻本身兴趣点分散， 相比用户对不同新闻的兴趣偏好， 新闻的及时性，热点性往往更加重要， 所以正好适用于发现热点，跟踪热点的趋势。
>   - 另外还具有推荐新信息的能力， 更有可能发现惊喜, 因为看的是人与人的相似性, 推出来的结果可能更有惊喜，可以发现用户潜在但自己尚未察觉的兴趣爱好。
> 
> （2）ItemCF
> 
> - 这个更适用于兴趣变化较为稳定的应用， 更接近于个性化的推荐， 适合**物品少，用户多，用户兴趣固定持久， 物品更新速度不是太快的场合**。
> - 比如推荐艺术品， 音乐， 电影。

**2.上面介绍的相似度计算方法有什么优劣之处？**

> cosine相似度计算简单方便，一般较为常用。但是，当用户的评分数据存在 bias 时，效果往往不那么好。
> 
> - 简而言之，就是不同用户评分的偏向不同。部分用户可能乐于给予好评，而部分用户习惯给予差评或者乱评分。
> - 这个时候，根据cosine 相似度计算出来的推荐结果效果会打折扣。
> 
> 举例来说明，如下图（`X,Y,Z` 表示物品，`d,e,f`表示用户）：
> 
> ![图片](http://ryluo.oss-cn-chengdu.aliyuncs.com/JavaWKvITKBhYOkfXrzs.png!thumbnail)
> 
> - 如果使用余弦相似度进行计算，用户 d 和 e 之间较为相似。但是实际上，用户 d 和 f 之间应该更加相似。只不过由于 d 倾向于打高分，e 倾向于打低分导致二者之间的余弦相似度更高。
> - 这种情况下，可以考虑使用皮尔逊相关系数计算用户之间的相似性关系。

#### FM（factor Machine，因子分解机）

**原理介绍**

https://zhuanlan.zhihu.com/p/58160982

https://tech.meituan.com/2016/03/03/deep-understanding-of-ffm-principles-and-practices.html

**实战讲解**

FM用于各阶段 https://zhuanlan.zhihu.com/p/343174108

demo（py)  https://github.com/gczr/FM

工业级demo (c/c++)  https://github.com/CastellanZhang/alphaFM

#### Item2vec

先复习Word2Vec

https://datawhalechina.github.io/fun-rec/#/ch02/ch2.1/ch2.1.2/word2vec 原理、推导、核心代码

https://zhuanlan.zhihu.com/p/89020340  Skip-Gram模型和负采样

##### Skip-gram & CBWO

##### Naive Softmax & 负采样Negative Sampling

注：注意到上图，中心词词向量为v_{c}*v**c*,而上下文词词向量为u_{o}*u**o*。也就是说每个词会对应两个词向量，**在词w做中心词时，使用v_{w}\*v\**w\*作为词向量，而在它做上下文词时，使用u_{w}\*u\**w\*作为词向量**。这样做的原因是为了求导等操作时计算上的简便。当整个模型训练完成后，我们既可以使用v_{w}*v**w*作为词w的词向量，也可以使用u_{w}*u**w*作为词w的词向量，亦或是将二者平均。

Q&A:

1. P(o|c)怎么表示？
2. 为何最小化损失函数能够得到良好表示的词向量dense word vector？

回答1：我们使用**中心词c和上下文词o的相似性**来计算P(o|c)*P*(*o*∣*c*)，更具体地，相似性由**词向量的点积**表示

使用词向量的点积表示P(o|c)的原因：1.计算简单 2.出现在一起的词向量意义相关，则希望它们相似

又P(o|c)是一个概率，所以我们在**整个语料库**上使用**softmax**将点积的值映射到概率

**Item2Vec** 的原理十分十分简单，它是基于 Skip-Gram 模型的物品向量训练方法。但又存在一些区别，如下：

- 词向量的训练是基于句子序列（sequence），但是物品向量的训练是基于物品集合（set）。
- 因此，物品向量的训练丢弃了空间、时间信息。

##### Item2vec实例——Airbnb召回

业务背景：

Airbnb 是全球最大的短租平台，包含了数百万种不同的房源。

Airbnb 平台 99% 的房源预订来自于**搜索排序**和**相似房源推荐**。

Embedding:

Airbnb 描述了两种 Embedding 的构建方法，分别为：

- 用于描述短期实时性的个性化特征 Embedding：**listing Embeddings**    (listing 表示房源)
- 用于描述长期的个性化特征 Embedding：**user-type & listing type Embeddings**
- 

Listing Embeddings 是基于用户的点击 session 学习得到的，用于表示房源的短期实时性特征。

建立多用户点击session，基于 Word2Vec 的 Skip-Gram 模型来学习不同 listing 的 Embedding 表示

改进点：

1.**正负样本集构建的改进**（使用 booked listing 作为全局上下文，负样本的选择新增了与其位于同一个 market 的 listing）

2.**Listing Embedding 的冷启动**（房主提供的房源信息，为其查找3个相似的 listing，并将它们 Embedding 的均值作为新 listing 的 Embedding表示）

##### User-type & Listing-type Embedding

除了挖掘 Listing 的短期兴趣特征表示外，还对 User 和 Listing 的长期兴趣特征表示进行了探索

长期兴趣的探索是基于 booking session（如上文，用户的历史预定序列，booked listing 表示用户在 session 中最终预定的房源）

遇到的问题：

- 预定本身就是一件低频率事件。booking sessions 数据量的大小远远小于 click sessions 
- 许多用户过去只预定了单个数量的房源，无法从长度为1的 session 中学习 Embedding
- 对于任何实体，要基于 context 学习到有意义的 Embedding，该实体至少在数据中出现5-10次。但平台上大多数 listing_ids 被预定的次数低于5-10次。
- 用户连续两次预定的时间间隔可能较长，在此期间用户的行为（如价格敏感点）偏好可能会发生改变（由于职业的变化）。

#### 特征工程概念补充

**feature coverage** https://datascience.stackexchange.com/questions/17121/definition-of-feature-coverage

**feature importance**





















### 经典排序模型



#### GBDT+LR

基本实现： https://github.com/lipengyuer/DataScience/tree/master/src/algoritm



CART算法：https://www.cnblogs.com/qiu-hua/p/14851247.html

 					https://zhuanlan.zhihu.com/p/32003259

​					 CART实例：https://www.cnblogs.com/qiu-hua/p/14851247.html



GBDT讲解  回归部分： https://mp.weixin.qq.com/s/Eh_YzmBDng5ChwSs2MUjxQ

​					分类部分： https://www.jianshu.com/p/f5e5db6b29f2

​					理论细节： https://blog.csdn.net/wuzhongqiang/article/details/108471107





#### FM

既可以用于召回也可以用于排序

显示建模二阶交叉

可以优化时空复杂度



#### FNN

![img](https://upload-images.jianshu.io/upload_images/23551183-f79550e72c599930.png?imageMogr2/auto-orient/strip|imageView2/2/format/webp)

![img](https://upload-images.jianshu.io/upload_images/23551183-afb740f6bc91e34e.png?imageMogr2/auto-orient/strip|imageView2/2/format/webp)





优点：

- 引入DNN对特征进行更高阶组合，减少特征工程，能在一定程度上增强FM的学习能力。这种尝试为后续深度推荐模型的发展提供了新的思路（相比模型效果而言，个人感觉这种融合思路意义更大）。

缺点：

- 两阶段训练模式，在应用过程中不方便，且模型能力受限于FM表征能力的上限。
- FNN专注于高阶组合特征，但是却没有将低阶特征纳入模型。

仔细分析下这种两阶段训练的方式，存在几个问题：

1）FM中进行特征组合，使用的是隐向量点积。将FM得到的隐向量移植到DNN中接入全连接层，全连接本质是将输入向量的所有元素进行加权求和，且不会对特征Field进行区分，也就是说FNN中高阶特征组合使用的是全部隐向量元素相加的方式。说到底，在理解特征组合的层面上FNN与FM是存在Gap的，而这一点也正是PNN对其进行改进的动力。

2）在神经网络的调参过程中，参数学习率是很重要的。况且FNN中底层参数是通过FM预训练而来，如果在进行反向传播更新参数的时候学习率过大，很容易将FM得到的信息抹去。个人理解，FNN至少应该采用Layer-wise learning rate，底层的学习率小一点，上层可以稍微大一点，在保留FM的二阶交叉信息的同时，在DNN上层进行更高阶的组合。



#### 补充—工程注意点

nn.embedding

nn.linear





#### PNN![image-20210308142624189](http://ryluo.oss-cn-chengdu.aliyuncs.com/%E5%9B%BE%E7%89%87image-20210308142624189.png)

https://datawhalechina.github.io/fun-rec/#/ch02/ch2.2/ch2.2.2/PNN

一共分为五层，其中除了Product Layer别的layer都是比较常规的处理方法。

模型中最重要的部分就是通过Product层对embedding特征进行交叉组合，Product层主要有线性部分和非线性部分组成，分别用l_z 和 l_p来表示。



理论推导和优化分析： https://zhuanlan.zhihu.com/p/89850560





#### Wide&Deep

![image-20200910214310877](http://ryluo.oss-cn-chengdu.aliyuncs.com/Javaimage-20200910214310877.png)

Q&A：

1. 为什么Wide部分要用L1 FTRL训练？
2. 为什么Deep部分不特别考虑稀疏性的问题？
3. 在你的应用场景中，哪些特征适合放在Wide侧，哪些特征适合放在Deep侧，为什么呢？

https://zhuanlan.zhihu.com/p/142958834

https://zhuanlan.zhihu.com/p/92279796

![img](https://pic3.zhimg.com/80/v2-0bd41080df368ff3767b42bb3bd2e882_720w.webp)

优点：

- 简单有效。结构简单易于理解，效果优异。目前仍在工业界广泛使用，也证明了该模型的有效性。
- 结构新颖。使用不同于以往的线性模型与DNN串行连接的方式，而将线性模型与DNN并行连接，同时兼顾模型的Memorization与Generalization。

缺点：

- Wide侧的特征工程仍无法避免。





#### DFM

本质上DeepFM是显式的针对特征各种组合建模：一阶特征与二阶交叉特征（FM部分）、高阶特征（DNN部分），最终将低阶到高阶的所有特征以并行的方式连接到一起。之前的模型或多或少都没有这么完备，三者至少缺其一。





#### DCN

该模型主要特点在于提出Cross network，用于特征的自动化交叉编码。传统DNN对于高阶特征的提取效率并不高，Cross Network通过调整结构层数能够构造出有限阶（bounded-degree）交叉特征，对特征进行显式交叉编码，在精简模型参数的同时有效的提高了模型的表征能力。



模型结构如下，共分为4个部分，分别为 Embedding and Stacking Layer（特征预处理输入）、Cross network（自动化特征显式交叉）、Deep network（特征隐式交叉）和Combination output Layer（输出）

![img](https://pic1.zhimg.com/80/v2-c3ca2754c02b4a2753aa6c47e7134bf8_720w.webp)

从模型结构上来看，DCN是将Wide&Deep中的Wide侧替换为Cross Network，利用该部分自动交叉特征的能力，模型无需进行额外的特征工程。

**当cross layer叠加 l 层时，交叉最高阶可以达到 l+1 阶，并且包含了所有的交叉组合，这是DCN的精妙之处。**

参考链接： https://zhuanlan.zhihu.com/p/96010464



#### 





列举几种模型进行完备性对比，结果如下所示。FNN模型与PNN模型将重心放在提取高阶特征信息，PNN中Product Layer精心构建低阶交叉特征信息（小于等于2阶），但是仅作为后续DNN的输入，并未将低阶特征与高阶特征并行连接。并且FNN需要进行参数预训练，模型构建时间开销较多。Wide&Deep模型将低阶与高阶特征同时建模，但是在Wide侧通常需要更多的特征工程工作。所以，整体对比下来DeepFM的完备性更高。

![img](https://pic3.zhimg.com/80/v2-275cc5be9cc045b19cda83a3169fa3a2_720w.webp)
